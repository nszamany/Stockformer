{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b74fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ns243\\AppData\\Local\\Temp\\ipykernel_2028\\1594062842.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(resp.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 503 tickers from Wikipedia.\n",
      "Completed 10/503 tickers...\n",
      "Completed 20/503 tickers...\n",
      "Completed 30/503 tickers...\n",
      "Completed 40/503 tickers...\n",
      "Completed 50/503 tickers...\n",
      "Completed 60/503 tickers...\n",
      "Completed 70/503 tickers...\n",
      "Completed 80/503 tickers...\n",
      "Completed 90/503 tickers...\n",
      "Completed 100/503 tickers...\n",
      "Completed 110/503 tickers...\n",
      "Completed 120/503 tickers...\n",
      "Completed 130/503 tickers...\n",
      "Completed 140/503 tickers...\n",
      "Completed 150/503 tickers...\n",
      "Completed 160/503 tickers...\n",
      "Completed 170/503 tickers...\n",
      "Completed 180/503 tickers...\n",
      "Completed 190/503 tickers...\n",
      "Completed 200/503 tickers...\n",
      "Completed 210/503 tickers...\n",
      "Completed 220/503 tickers...\n",
      "Completed 230/503 tickers...\n",
      "Completed 240/503 tickers...\n",
      "Completed 250/503 tickers...\n",
      "Completed 260/503 tickers...\n",
      "Completed 270/503 tickers...\n",
      "Completed 280/503 tickers...\n",
      "Completed 290/503 tickers...\n",
      "Completed 300/503 tickers...\n",
      "Completed 310/503 tickers...\n",
      "Completed 320/503 tickers...\n",
      "Completed 330/503 tickers...\n",
      "Completed 340/503 tickers...\n",
      "Completed 350/503 tickers...\n",
      "Completed 360/503 tickers...\n",
      "Completed 370/503 tickers...\n",
      "Completed 380/503 tickers...\n",
      "Completed 390/503 tickers...\n",
      "Completed 400/503 tickers...\n",
      "Completed 410/503 tickers...\n",
      "Completed 420/503 tickers...\n",
      "Completed 430/503 tickers...\n",
      "Completed 440/503 tickers...\n",
      "Completed 450/503 tickers...\n",
      "Completed 460/503 tickers...\n",
      "Completed 470/503 tickers...\n",
      "Completed 480/503 tickers...\n",
      "Completed 490/503 tickers...\n",
      "Completed 500/503 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ns243\\AppData\\Local\\Temp\\ipykernel_2028\\1594062842.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  panel = pd.concat(results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved consolidated S&P500 dataset to:\n",
      "C:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Data\\SP500_Consolidated.zip\n",
      "Done in 2.36 min\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTDIR = r\"C:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Data\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "SAVE_PATH = os.path.join(OUTDIR, \"SP500_Consolidated.zip\")\n",
    "YEARS = 10\n",
    "WORKERS = 10\n",
    "# ==========================================\n",
    "\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Fetch S&P500 tickers from Wikipedia using requests (avoids 403).\"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    resp = requests.get(url, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    tables = pd.read_html(resp.text)\n",
    "    tickers = tables[0][\"Symbol\"].tolist()\n",
    "    mapping = {\"BRK.B\": \"BRK-B\", \"BF.B\": \"BF-B\"}\n",
    "    tickers = [mapping.get(t, t) for t in tickers]\n",
    "    return tickers\n",
    "\n",
    "def fetch_one(ticker):\n",
    "    \"\"\"Fetch OHLCV + fundamentals for one ticker.\"\"\"\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        hist = t.history(period=f\"{YEARS}y\", auto_adjust=False)\n",
    "        if hist.empty:\n",
    "            return None\n",
    "\n",
    "        hist = hist.rename(columns={\n",
    "            \"Open\": \"Open\", \"High\": \"High\", \"Low\": \"Low\",\n",
    "            \"Close\": \"Close\", \"Volume\": \"Volume\"\n",
    "        })\n",
    "        hist[\"Ticker\"] = ticker\n",
    "\n",
    "        # fundamentals\n",
    "        info = t.info\n",
    "        fundamentals = {\n",
    "            \"PE_TTM\": info.get(\"trailingPE\"),\n",
    "            \"PB_Ratio\": info.get(\"priceToBook\"),\n",
    "            \"PS_Ratio\": info.get(\"priceToSalesTrailing12Months\"),\n",
    "            \"EPS_TTM\": info.get(\"trailingEps\"),\n",
    "            \"DividendYield\": info.get(\"dividendYield\"),\n",
    "            \"ROE\": info.get(\"returnOnEquity\"),\n",
    "            \"SharesOutstanding\": info.get(\"sharesOutstanding\"),\n",
    "        }\n",
    "        for k, v in fundamentals.items():\n",
    "            hist[k] = v\n",
    "\n",
    "        # derived\n",
    "        hist[\"MarketCap\"] = hist[\"Close\"] * hist[\"SharesOutstanding\"]\n",
    "        hist[\"TurnoverProxy\"] = hist[\"Volume\"] / hist[\"SharesOutstanding\"]\n",
    "\n",
    "        hist = hist.reset_index().rename(columns={\"Date\": \"date\"})\n",
    "        hist = hist.set_index([\"date\", \"Ticker\"]).sort_index()\n",
    "        return hist\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_sp500_consolidated():\n",
    "    tickers = get_sp500_tickers()\n",
    "    print(f\"Fetched {len(tickers)} tickers from Wikipedia.\")\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "        futures = {executor.submit(fetch_one, t): t for t in tickers}\n",
    "        for i, f in enumerate(as_completed(futures), 1):\n",
    "            df = f.result()\n",
    "            if df is not None:\n",
    "                results.append(df)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Completed {i}/{len(tickers)} tickers...\")\n",
    "\n",
    "    if not results:\n",
    "        raise RuntimeError(\"No data fetched. Check yfinance connectivity or rate limits.\")\n",
    "\n",
    "    panel = pd.concat(results)\n",
    "    panel = panel.groupby(\"Ticker\").apply(lambda x: x.ffill().bfill()).reset_index(level=0, drop=True)\n",
    "    panel = panel.sort_index()\n",
    "\n",
    "    panel.to_csv(SAVE_PATH, compression=\"zip\")\n",
    "    print(f\"\\nSaved consolidated S&P500 dataset to:\\n{SAVE_PATH}\")\n",
    "    return panel\n",
    "\n",
    "# ================= RUN =================\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    try:\n",
    "        df = build_sp500_consolidated()\n",
    "        print(f\"Done in {(time.time()-start)/60:.2f} min\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59edf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SP500_Consolidated']\n",
      "Shape of data: (1231178, 19)\n",
      "Columns: ['date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits', 'PE_TTM', 'PB_Ratio', 'PS_Ratio', 'EPS_TTM', 'DividendYield', 'ROE', 'SharesOutstanding', 'MarketCap', 'TurnoverProxy']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Data\\SP500_Consolidated.zip\"\n",
    "\n",
    "# Look inside the ZIP to find the CSV name\n",
    "with zipfile.ZipFile(SAVE_PATH, 'r') as z:\n",
    "    print(z.namelist())  # list of files inside\n",
    "    csv_name = z.namelist()[0]  # first (and probably only) file\n",
    "    with z.open(csv_name) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "print(\"Shape of data:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
