{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f41836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Found 101 factor files. Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Factor files: 100%|██████████| 101/101 [28:13<00:00, 16.76s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging factor long tables...\n",
      "Saved merged factors to c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\factors_merged.csv\n",
      "Reading price file...\n",
      "Saved market cap pivot to c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\df_ltsz.csv (values are market_cap, not log).\n",
      "503 unique tickers found; 503 missing from cache. Fetching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching industry: 100%|██████████| 503/503 [07:02<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ticker industry cache to c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\ticker_industry_map.csv\n",
      "Saved industry dummies to c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\industry_dummies.csv\n",
      "All done. Outputs are in the './output' directory.\n",
      "Files produced:\n",
      " - c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\factors_merged.csv\n",
      " - c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\df_ltsz.csv\n",
      " - c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\industry_dummies.csv\n",
      " - c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\ticker_industry_map.csv\n",
      "\n",
      "Notes:\n",
      " - If yfinance fetches fail for some tickers, check ticker format (e.g. BRK-B vs BRK.B or different Yahoo symbols).\n",
      " - The script caches industry lookups; if you need to refresh, delete ticker_industry_map.csv and re-run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data_prep_sp500.py\n",
    "\"\"\"\n",
    "Prepare S&P500 factor and price data for neutralization.\n",
    "\n",
    "Outputs (in ./output/):\n",
    " - factors_merged.csv        : merged long-format factor table (datetime, instrument, <factor columns...>)\n",
    " - df_ltsz.csv              : pivot table of market cap (index=date, columns=code) -- floats\n",
    " - industry_dummies.csv     : industry dummy variables per ticker (index=code)\n",
    " - ticker_industry_map.csv  : ticker -> industry string map (cached)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Config - change if needed\n",
    "# -------------------------\n",
    "FACTORS_FOLDER = r\"C:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Data\\alpha158_processed\"\n",
    "PRICE_FILE = r\"C:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Data\\df_sp500.csv\"\n",
    "OUTPUT_DIR = os.path.abspath(\"./output\")\n",
    "TICKER_INDUSTRY_CACHE = os.path.join(OUTPUT_DIR, \"ticker_industry_map.csv\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "\n",
    "def parse_timestamp_series(s):\n",
    "    \"\"\"Parse datetimes, keep timezone info, convert to UTC and normalize to midnight UTC if needed.\n",
    "       Returns pandas.DatetimeIndex (UTC tz-aware) and original formatted string column for saving.\n",
    "    \"\"\"\n",
    "    # coerce errors, keep timezone where present, convert to UTC\n",
    "    dt = pd.to_datetime(s, errors='coerce', utc=True)\n",
    "    # Some data might be NaT: keep them as NaT\n",
    "    return dt\n",
    "\n",
    "def infer_factor_name(filepath):\n",
    "    \"\"\"Create a sane factor name from filename (without extension).\"\"\"\n",
    "    name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    # sanitize\n",
    "    name = name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    return f\"factor_{name}\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Merge factor CSVs\n",
    "# -------------------------\n",
    "def merge_factors(folder):\n",
    "    \"\"\"\n",
    "    Each factor CSV assumed to have:\n",
    "      - a 'date' column\n",
    "      - remaining columns are tickers (one column per ticker)\n",
    "    Produces a long-format DataFrame: columns = ['datetime','instrument', <factor columns>...]\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSVs found in {folder}\")\n",
    "    print(f\"Found {len(csv_files)} factor files. Merging...\")\n",
    "\n",
    "    factors_long_list = []\n",
    "    factor_names = []\n",
    "    for fp in tqdm(csv_files, desc=\"Factor files\"):\n",
    "        fname = infer_factor_name(fp)\n",
    "        factor_names.append(fname)\n",
    "        df = pd.read_csv(fp, dtype=str)  # read as str to avoid unexpected dtype issues\n",
    "        if 'date' not in df.columns:\n",
    "            raise ValueError(f\"file {fp} missing 'date' column\")\n",
    "        # parse datetime column\n",
    "        df['datetime'] = parse_timestamp_series(df['date'])\n",
    "        df = df.drop(columns=['date'])\n",
    "\n",
    "        # melt: columns except datetime -> instrument, value\n",
    "        val_cols = [c for c in df.columns if c != 'datetime']\n",
    "        df_m = df.melt(id_vars=['datetime'], value_vars=val_cols,\n",
    "                       var_name='instrument', value_name=fname)\n",
    "        # coerce numeric\n",
    "        df_m[fname] = pd.to_numeric(df_m[fname], errors='coerce')\n",
    "        factors_long_list.append(df_m)\n",
    "\n",
    "    # Merge all on ['datetime','instrument']\n",
    "    print(\"Merging factor long tables...\")\n",
    "    merged = factors_long_list[0]\n",
    "    for dfm in factors_long_list[1:]:\n",
    "        merged = merged.merge(dfm, how='outer', on=['datetime', 'instrument'])\n",
    "\n",
    "    # Ensure consistent ordering and drop rows with no factor values at all\n",
    "    factor_cols = [f for f in merged.columns if f not in ['datetime','instrument']]\n",
    "    merged = merged.dropna(axis=0, how='all', subset=factor_cols)\n",
    "    merged = merged.sort_values(['datetime','instrument']).reset_index(drop=True)\n",
    "\n",
    "    # Save a wide-ish format: columns datetime,instrument,<factors...>\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"factors_merged.csv\")\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"Saved merged factors to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# -------------------------\n",
    "# 2) Prepare price & market cap\n",
    "# -------------------------\n",
    "def prepare_price_marketcap(price_csv):\n",
    "    \"\"\"\n",
    "    Reads df_sp500.csv which should have columns similar to:\n",
    "    date, stock_code, open, high, low, close, factor, change, volume, money, shares_out\n",
    "    Produces:\n",
    "     - df_ltsz.csv : pivoted market cap (index: date, columns: code)\n",
    "    \"\"\"\n",
    "    print(\"Reading price file...\")\n",
    "    df = pd.read_csv(price_csv)\n",
    "    # Rename likely columns to canonical ones if needed\n",
    "    # Accept either 'stock_code' or 'code' or 'ticker'\n",
    "    for cand in ['stock_code', 'code', 'ticker', 'instrument']:\n",
    "        if cand in df.columns:\n",
    "            df = df.rename(columns={cand: 'code'})\n",
    "            break\n",
    "    if 'date' not in df.columns or 'code' not in df.columns:\n",
    "        raise ValueError(\"price file must contain 'date' and a ticker column (stock_code/code/ticker)\")\n",
    "\n",
    "    df['datetime'] = parse_timestamp_series(df['date'])\n",
    "    df = df.drop(columns=['date'])\n",
    "\n",
    "    # ensure numeric columns exist\n",
    "    if 'close' not in df.columns:\n",
    "        raise ValueError(\"price file must contain 'close' column\")\n",
    "    # shares_out must exist to compute market cap\n",
    "    if 'shares_out' not in df.columns:\n",
    "        raise ValueError(\"price file must contain 'shares_out' column to compute market cap\")\n",
    "\n",
    "    # coerce numeric\n",
    "    df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "    df['shares_out'] = pd.to_numeric(df['shares_out'], errors='coerce')\n",
    "\n",
    "    # compute market cap\n",
    "    df['market_cap'] = df['close'] * df['shares_out']\n",
    "\n",
    "    # drop rows missing code or datetime\n",
    "    df = df.dropna(subset=['code','datetime'])\n",
    "\n",
    "    # pivot to create market cap table (index: datetime, columns: code)\n",
    "    df_pivot = df.pivot_table(index='datetime', columns='code', values='market_cap', aggfunc='first')\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"df_ltsz.csv\")\n",
    "    # save raw market cap (not log), index will be timestamps in ISO format\n",
    "    df_pivot.to_csv(out_path)\n",
    "    print(f\"Saved market cap pivot to {out_path} (values are market_cap, not log).\")\n",
    "    return out_path\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fetch industries (yfinance)\n",
    "# -------------------------\n",
    "def fetch_industries_for_tickers(tickers, cache_path=TICKER_INDUSTRY_CACHE):\n",
    "    \"\"\"\n",
    "    Uses yfinance to fetch the 'industry' string for each ticker.\n",
    "    Caches the mapping to disk so repeated runs are fast.\n",
    "    Returns DataFrame: index=ticker, columns=['industry','sector']\n",
    "    \"\"\"\n",
    "    # try to load cache\n",
    "    if os.path.exists(cache_path):\n",
    "        df_cache = pd.read_csv(cache_path, index_col=0)\n",
    "    else:\n",
    "        df_cache = pd.DataFrame(columns=['industry','sector'])\n",
    "\n",
    "    tickers = sorted(set([t.upper() for t in tickers if pd.notna(t)]))\n",
    "    new_needed = [t for t in tickers if t not in df_cache.index]\n",
    "    print(f\"{len(tickers)} unique tickers found; {len(new_needed)} missing from cache. Fetching...\")\n",
    "\n",
    "    # fetch in batches: yfinance supports Tickers(t1 t2 ...)\n",
    "    # but we'll loop to avoid occasional partial failures\n",
    "    for t in tqdm(new_needed, desc=\"Fetching industry\"):\n",
    "        try:\n",
    "            tk = yf.Ticker(t)\n",
    "            info = tk.info\n",
    "            industry = info.get('industry') or info.get('Industry') or None\n",
    "            sector = info.get('sector') or info.get('Sector') or None\n",
    "        except Exception as e:\n",
    "            # fail gracefully\n",
    "            industry = None\n",
    "            sector = None\n",
    "        df_cache.loc[t] = [industry, sector]\n",
    "\n",
    "    # fill NaN industries with 'Unknown'\n",
    "    df_cache['industry'] = df_cache['industry'].fillna('Unknown')\n",
    "    df_cache['sector'] = df_cache['sector'].fillna('Unknown')\n",
    "\n",
    "    # save cache\n",
    "    df_cache.to_csv(cache_path)\n",
    "    print(f\"Saved ticker industry cache to {cache_path}\")\n",
    "    return df_cache\n",
    "\n",
    "def build_industry_dummies(industry_map_df):\n",
    "    \"\"\"\n",
    "    From ticker->industry mapping DataFrame, build one-hot dummies.\n",
    "    Returns DataFrame indexed by ticker with dummy columns.\n",
    "    \"\"\"\n",
    "    df = industry_map_df.copy()\n",
    "    df['industry'] = df['industry'].fillna('Unknown')\n",
    "    dummies = pd.get_dummies(df['industry'], prefix='ind')\n",
    "    dummies.index = df.index\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"industry_dummies.csv\")\n",
    "    dummies.to_csv(out_path)\n",
    "    print(f\"Saved industry dummies to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting data preparation...\")\n",
    "\n",
    "    merged_factors_path = merge_factors(FACTORS_FOLDER)\n",
    "    df_ltsz_path = prepare_price_marketcap(PRICE_FILE)\n",
    "\n",
    "    # read merged factors to get ticker universe\n",
    "    merged = pd.read_csv(merged_factors_path, parse_dates=['datetime'])\n",
    "    tickers = merged['instrument'].unique().tolist()\n",
    "\n",
    "    industry_map = fetch_industries_for_tickers(tickers)\n",
    "    industry_dummies_path = build_industry_dummies(industry_map)\n",
    "\n",
    "    print(\"All done. Outputs are in the './output' directory.\")\n",
    "    print(\"Files produced:\")\n",
    "    print(\" -\", merged_factors_path)\n",
    "    print(\" -\", df_ltsz_path)\n",
    "    print(\" -\", industry_dummies_path)\n",
    "    print(\" -\", TICKER_INDUSTRY_CACHE)\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\" - If yfinance fetches fail for some tickers, check ticker format (e.g. BRK-B vs BRK.B or different Yahoo symbols).\")\n",
    "    print(\" - The script caches industry lookups; if you need to refresh, delete ticker_industry_map.csv and re-run.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
