{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc5832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inputs...\n",
      "Running neutralization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neutralizing dates: 100%|██████████| 2515/2515 [57:18<00:00,  1.37s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved neutralized factors to c:\\Users\\ns243\\Documents\\Academic\\AI Master\\Internship\\Codes\\output\\factors_neutralized.csv\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# neutralize_factors_sp500.py\n",
    "\"\"\"\n",
    "Neutralize factors by market cap (log) and industry dummies, per date.\n",
    "\n",
    "Inputs (expected in ./output/):\n",
    " - factors_merged.csv       : merged factors (datetime,instrument,factor_... )\n",
    " - df_ltsz.csv              : market_cap pivot (index=datetime, columns=code)\n",
    " - industry_dummies.csv     : industry dummy matrix (index=code)\n",
    "\n",
    "Output:\n",
    " - factors_neutralized.csv  : neutralized & standardized factors with columns:\n",
    "     datetime, instrument, <factor columns (neutralized)>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "\n",
    "OUTPUT_DIR = os.path.abspath(\"./output\")\n",
    "FACTORS_MERGED = os.path.join(OUTPUT_DIR, \"factors_merged.csv\")\n",
    "DF_LTSZ = os.path.join(OUTPUT_DIR, \"df_ltsz.csv\")\n",
    "INDUS = os.path.join(OUTPUT_DIR, \"industry_dummies.csv\")\n",
    "OUT_NEUT = os.path.join(OUTPUT_DIR, \"factors_neutralized.csv\")\n",
    "\n",
    "def load_inputs():\n",
    "    print(\"Loading inputs...\")\n",
    "    fac = pd.read_csv(FACTORS_MERGED, parse_dates=['datetime'])\n",
    "    ltsz = pd.read_csv(DF_LTSZ, index_col=0, parse_dates=True)\n",
    "    indus = pd.read_csv(INDUS, index_col=0)\n",
    "    # ensure index types\n",
    "    ltsz.index = pd.to_datetime(ltsz.index, utc=True)\n",
    "    return fac, ltsz, indus\n",
    "\n",
    "def neutralize_per_date(fac_df, ltsz_pivot, indus_df):\n",
    "    \"\"\"\n",
    "    fac_df: long format DataFrame with columns ['datetime','instrument',factor_...]\n",
    "    ltsz_pivot: pivoted market_cap DataFrame index=datetime columns=tickers\n",
    "    indus_df: index=tickers -> dummy cols\n",
    "\n",
    "    Returns: DataFrame with index same as fac_df (grouped rows) containing neutralized factor cols plus datetime & instrument\n",
    "    \"\"\"\n",
    "    factor_cols = [c for c in fac_df.columns if c not in ['datetime','instrument']]\n",
    "    out_rows = []\n",
    "    # group by datetime\n",
    "    fac_df['datetime'] = pd.to_datetime(fac_df['datetime'], utc=True)\n",
    "    grouped = fac_df.groupby('datetime')\n",
    "    all_dates = sorted(fac_df['datetime'].unique())\n",
    "\n",
    "    for date in tqdm(all_dates, desc=\"Neutralizing dates\"):\n",
    "        group = grouped.get_group(date).set_index('instrument')\n",
    "        # get market cap series for this date (align tickers)\n",
    "        try:\n",
    "            mc = ltsz_pivot.loc[date]\n",
    "        except KeyError:\n",
    "            # no market cap data for this date -> skip (or create NaNs)\n",
    "            mc = pd.Series(index=group.index, dtype=float)\n",
    "\n",
    "        # log market cap; coerce nonpositive to NaN\n",
    "        mc = pd.to_numeric(mc, errors='coerce')\n",
    "        mc = mc.replace(0, np.nan)\n",
    "        lmc = np.log(mc)\n",
    "        lmc.name = 'log_mktcap'\n",
    "\n",
    "        # assemble industry dummies for the tickers in this group\n",
    "        ind_sub = indus_df.reindex(group.index).fillna(0)\n",
    "\n",
    "        # for each factor col, run OLS: factor ~ log_mktcap + industry_dummies\n",
    "        result_factors = {}\n",
    "        for fcol in factor_cols:\n",
    "            y = group[fcol].to_frame(name='y')\n",
    "            # if column entirely nan -> keep as NaN\n",
    "            if y['y'].notna().sum() == 0:\n",
    "                result_factors[fcol] = pd.Series(index=group.index, dtype=float)\n",
    "                continue\n",
    "\n",
    "            # build reg matrix: concat lmc and ind_sub\n",
    "            X = pd.concat([lmc.reindex(group.index), ind_sub], axis=1)\n",
    "            X = X.astype(float)\n",
    "            # drop columns that are all nan\n",
    "            X = X.loc[:, X.notna().any(axis=0)]\n",
    "            # drop rows that have all X missing or y missing\n",
    "            dfjoint = pd.concat([y, X], axis=1)\n",
    "            dfjoint = dfjoint.dropna(subset=['y'], how='all')  # keep rows where y exists\n",
    "            # If after dropping there's nothing, skip\n",
    "            if dfjoint.shape[0] == 0 or dfjoint.iloc[:,1:].shape[1] == 0:\n",
    "                # no regressors available -> just zscore y\n",
    "                resid = y['y']\n",
    "            else:\n",
    "                y_reg = dfjoint['y']\n",
    "                X_reg = dfjoint.iloc[:,1:]  # regressors\n",
    "                # add const? No const is fine since we want residuals relative to regressors\n",
    "                # but statsmodels requires explicit constant if desired. We'll keep no constant to mimic earlier code.\n",
    "                try:\n",
    "                    model = sm.OLS(y_reg, X_reg, missing='drop')\n",
    "                    res = model.fit()\n",
    "                    fitted = res.fittedvalues\n",
    "                    resid = y['y'].copy()\n",
    "                    # subtract fitted only for indices present\n",
    "                    resid.loc[fitted.index] = y.loc[fitted.index, fcol] - fitted\n",
    "                except Exception:\n",
    "                    # fallback: if regression fails, set resid to y\n",
    "                    resid = y['y']\n",
    "\n",
    "            # standardize residuals (z-score)\n",
    "            resid = resid.replace([np.inf, -np.inf], np.nan)\n",
    "            if resid.notna().sum() >= 2:\n",
    "                resid = (resid - resid.mean()) / resid.std(ddof=0)\n",
    "            else:\n",
    "                resid = resid * np.nan  # insufficient data\n",
    "\n",
    "            result_factors[fcol] = resid\n",
    "\n",
    "        # combine into DataFrame for this date\n",
    "        df_out_date = pd.DataFrame(result_factors)\n",
    "        df_out_date.index.name = 'instrument'\n",
    "        df_out_date = df_out_date.reset_index()\n",
    "        df_out_date['datetime'] = date\n",
    "        out_rows.append(df_out_date)\n",
    "\n",
    "    out_df = pd.concat(out_rows, ignore_index=True, sort=False)\n",
    "    # reorder columns\n",
    "    cols = ['datetime','instrument'] + factor_cols\n",
    "    cols = [c for c in cols if c in out_df.columns]\n",
    "    out_df = out_df[cols]\n",
    "    return out_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fac, ltsz, indus = load_inputs()\n",
    "    print(\"Running neutralization...\")\n",
    "    neut = neutralize_per_date(fac, ltsz, indus)\n",
    "    neut.to_csv(OUT_NEUT, index=False)\n",
    "    print(\"Saved neutralized factors to\", OUT_NEUT)\n",
    "    print(\"Finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
